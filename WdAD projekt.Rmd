---
title: "WdAD"
author: "MichaÅ‚ Stadnik"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Libraries
```{r}
library(dplyr)
library(tidyr)
```


```{r}
path = "D:\\Programowanie\\R\\WdAD\\WdAD_projekt_wikipedia-main\\data\\math_19_10_2025.csv"

data = read.csv(path, row.names = "X") #In the "X" column are indexes

data = subset(data, select = -image_titles) #I think image_titles is useless

```

Two interesting findings:
```{r}
data |> 
  filter(
    word_count == 0
  )
```

These are the pages which was redirected to other (I checked it for first 10). I think they are useless too. Let's note their summary are empty.

```{r}
data <- data |> 
  filter(
    word_count != 0
  )
```

Let's check if there are other rows with empty summary.

```{r}
data |> 
  filter(
    summary == ""
  )
```
So all pages with empty summary had 0 word_count.
```{r}
data |> 
  filter(
    word_count <= num_links_internal & word_count > 0
  )
```

Mainly they are a list articles (articles in which is a list of other articles) or short articles with lists of something, but sometimes for some reason there are other articles (see "Trapping region" or "European Society for Mathematics and the Arts"). These other articles are also short, but contain a big cathegory block, and probably links in this block was counted. Interesting think happens for example in "Graduate Studies in Mathematics", where there is list with 251 positions, but in data set it is only 61 ordinary words and 120 internal links. Here should be at least 251 links, because each position has at least one link. 

Here we see that links are counted seperatly than words. It is worth to note that links can contain two or more words, and in that case they are also counted as one link.

```{r}
summary(data)
```
Let's note quite large maximum values in `mo_page_views`, `clicks_in`, `clicks_out` and `clicks_per_view` (which is directly corelated with first 3 variables). Let`s check this outliers.
```{r}
data |> 
  slice_max(mo_page_views, n=20, with_ties = TRUE) |> 
  select(
    title, mo_page_views , clicks_in, clicks_out, clicks_per_view
  )
```
Good question is, why Sagrada Familia has category "Matematics"? The answer is, it has category "Mathematics and art". So in this dataset are articles with at least one category which contains word "Mathematics". However it is not a big outlier. We can see, that `mo_page_views` has heavy tails, not a few large outliers. I think we should take a logharitm of this column.
```{r}
data |> 
  slice_max(clicks_in, n = 20, with_ties = TRUE) |> 
  select(
    title, mo_page_views , clicks_in, clicks_out, clicks_per_view
  )
```
Same results here. Also this column should be logharitmed. "Gifted" is a movie about mathematical genius, so its presents here is justiced.
```{r}
data |> 
  slice_max(clicks_out, n = 20, with_ties = TRUE) |> 
  select(
    title, mo_page_views , clicks_in, clicks_out, clicks_per_view
  )
```
Here are outliers. Let's note only first 4 rows are above 20.000, and only 10 next are above 10.000. Let's delete them and check summary.
```{r}
data <- data |> 
  filter(
    clicks_out < 10000
  )

summary(data$clicks_out)
```
Ohh, they also should be logharitmed.
```{r}
data |> 
  slice_max(clicks_per_view, n = 20, with_ties = TRUE) |> 
  select(
    title, mo_page_views , clicks_in, clicks_out, clicks_per_view
  )
```
Here "2 + 2 = 5" is definitely an outlier. We should consider delete it. Let's look at summary without this one:
```{r}
data <- data |> 
  filter(
    title != "2 + 2 = 5"
  )

summary(data$clicks_per_view)
```
Let's logharitm all numeric columns by the way.
```{r}
data <- data %>%
  mutate(across(where(is.numeric), log1p, .names = "log_{.col}"))

log_variables <- data[c('log_mo_page_views', 'log_clicks_in', 'log_clicks_out', 'log_clicks_per_view')]

summary(log_variables)
```
Let's see histograms
```{r}
numeric_cols <- names(data)[sapply(data, is.numeric)]

for (i in numeric_cols){
  hist(data[[i]], xlab = i)
}
```

Histograms of `log_mo_page_views`, `log_clicks_in` and `log_clicks_out` looks like closly to normal distributions, but it demands deleting of 0-values. However `clicks_per_view` doesn't behave normaly, even after logarithm.

Let's check how many are 0-values in this variables.
```{r}
zeros <- colSums(log_variables == 0, na.rm = TRUE)

zeros
zeros/nrow(log_variables) * 100
```
It looks like, the number of pages with no views is marginal, while pages with no clicks in are 7% of total and pages with no clicks are almost 1/3 of total. Let's figure out what happen exactly with these data.

```{r}
zero_data <- data |> 
  filter(
    log_clicks_out == 0
  )

zero_log_variables <- zero_data[c('log_mo_page_views', 'log_clicks_in', 'log_clicks_out', 'log_clicks_per_view')]

colSums(zero_log_variables == 0, na.rm = TRUE)
```
We can see, that the numbers of 0 entries are the same, as for the whole dataset. It is expected, because if no one views the page then nobody could click some link on that page. However for pages in which nobody clicks in it is possibly to somebody clicks out, but it looks like in this dataset these situations don't occur.

Let's find out some other patterns in these 0 data.
```{r}
summary(zero_data)
```

Mostly variables have lower means than in the whole dataset. Especially `mo_page_views` and `clicks_in`, however it is also visible that they are mainly shorter articles with lower number of links. Maybe we should consider two-step alghorithm, where on the first step we classify whether article has 0 `clicks_per_view` ratio or not, and if our method says it differs from zero, we try predict how much it is?

Because, `clicks_per_view` is calculated as `clicks_out`/`mo_page_views`, and `clicks_in` can also be highly corelated with this variables in order to prevent data leakage, we will train our models on data without this variables. We also create seperate dataframes with data and logharitmed versions
```{r}
leakage_cols <- c("clicks_in", "clicks_out", "mo_page_views", "log_clicks_in", "log_clicks_out", "log_mo_page_views")

data_for_models <- data |> 
  select(-any_of(leakage_cols)) %>%
  select(-starts_with("log_"))

log_data_for_models <- data_for_models %>%
  mutate(across(
    .cols = where(is.numeric) & !clicks_per_view, 
    .fns = log1p,
    .names = "log_{.col}"
  ))
```
